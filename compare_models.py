#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
compare_models.py
-----------------
Standalone script to compare outputs from your LSTM and CNN pipelines.
It auto-discovers result CSVs or accepts explicit paths and produces:
  - A unified, cleaned results table per model
  - Per‑ticker "champion" rows (best combo per model by a chosen metric)
  - A head‑to‑head per‑ticker comparison (CNN vs LSTM)
  - Overall summary stats
  - Matplotlib charts (no seaborn) saved to output/compare_plots/
  - CSV artifacts saved to output/compare/

USAGE
-----
python compare_models.py \
    --lstm results/lstm_results.csv \
    --cnn  results/cnn_results.csv \
    --select_metric DA_test

If arguments are omitted, the script will:
  1) Try to load paths from config/config.yaml (keys: result_file_lstm, result_file_cnn)
  2) Fallback to results/lstm_results.csv and results/cnn_results.csv
  3) As a last resort, try to auto-discover files by globbing **/lstm_results*.csv etc.

METRICS
-------
We normalize column names and look for the following (case-insensitive):
    ticker, features, k, DA_val, RMSE_val, MAE_val, MAPE_val,
    DA_test, RMSE_test, MAE_test, MAPE_test, PnL, PnL_ratio, search_mode
Only a subset may be present; the comparison adapts accordingly.

OUTPUT
------
output/compare/
  unified_lstm.csv, unified_cnn.csv
  champions_lstm.csv, champions_cnn.csv
  head_to_head_by_ticker.csv
  summary_stats.csv
output/compare_plots/
  mean_metric_bars.png
  scatter_DA_test_cnn_vs_lstm.png  (if DA_test available)

Author: Generated by ChatGPT
"""
import argparse, sys, json, math
from pathlib import Path
import itertools
import numpy as np
import pandas as pd

# Matplotlib only (no seaborn)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

try:
    import yaml
    _HAS_YAML = True
except Exception:
    _HAS_YAML = False

# ------------------------------
# Utilities
# ------------------------------
STD_NAME_MAP = {
    # lowercased source -> standard name
    "ticker": "ticker",
    "isin": "ticker",
    "symbol": "ticker",
    "features": "features",
    "k": "k",
    "da_val": "DA_val",
    "rmse_val": "RMSE_val",
    "mae_val": "MAE_val",
    "mape_val": "MAPE_val",
    "da_test": "DA_test",
    "rmse_test": "RMSE_test",
    "mae_test": "MAE_test",
    "mape_test": "MAPE_test",
    "pnl": "PnL",
    "pnl_ratio": "PnL_ratio",
    "search_mode": "search_mode",
    "window": "window",
    "run_id": "run_id",
    "score": "score",           # sometimes in your CSVs
    "score_metric": "score_metric",
}

METRIC_ORDER = ["DA_test", "RMSE_test", "MAE_test", "MAPE_test", "PnL", "PnL_ratio",
                "DA_val", "RMSE_val", "MAE_val", "MAPE_val"]

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    colmap = {}
    for c in df.columns:
        key = str(c).strip().lower()
        std = STD_NAME_MAP.get(key, None)
        if std is None:
            # Try to handle variants like 'DA (test)' etc.
            k2 = key.replace(" ", "").replace("(", "_").replace(")", "").replace("-", "_")
            std = STD_NAME_MAP.get(k2, c)
        colmap[c] = std if std else c
    df = df.rename(columns=colmap)
    return df

def _find_paths(cli_path: str, yaml_key: str, fallback: str, pattern: str):
    """Resolve a result file path in this priority:
       1) CLI path (if given and exists)
       2) config/config.yaml (if key present and exists)
       3) fallback path
       4) first match of glob pattern (recursive)
    """
    # CLI
    if cli_path:
        p = Path(cli_path)
        if p.exists():
            return p
        else:
            print(f"[warn] Provided path not found: {p}")

    # YAML
    if _HAS_YAML:
        cfg = Path("config/config.yaml")
        if cfg.exists():
            try:
                with cfg.open("r", encoding="utf-8") as f:
                    y = yaml.safe_load(f) or {}
                val = y
                for k in yaml_key.split("."):
                    if isinstance(val, dict) and k in val:
                        val = val[k]
                    else:
                        val = None
                        break
                if isinstance(val, str):
                    p = Path(val)
                    if p.exists():
                        return p
            except Exception as e:
                print(f"[warn] Could not read YAML: {e}")

    # Fallback
    p = Path(fallback)
    if p.exists():
        return p

    # Glob
    matches = list(Path(".").rglob(pattern))
    if matches:
        return matches[0]

    return None

def pick_champions(df, select_metric="DA_test"):
    """Pick the best row per (ticker) according to select_metric.
       For 'higher is better' metrics (DA_test, PnL, etc.) we maximize;
       for error metrics (RMSE, MAE, MAPE) we minimize.
    """
    if df is None or df.empty or select_metric not in df.columns:
        return pd.DataFrame()

    higher_is_better = {"DA_test", "DA_val", "PnL", "PnL_ratio"}
    lower_is_better  = {"RMSE_test", "MAE_test", "MAPE_test",
                        "RMSE_val", "MAE_val", "MAPE_val"}

    def pick_fn(group):
        if select_metric in higher_is_better:
            idx = group[select_metric].astype(float).idxmax()
        elif select_metric in lower_is_better:
            idx = group[select_metric].astype(float).idxmin()
        else:
            # default to maximize
            idx = group[select_metric].astype(float).idxmax()
        return group.loc[idx]

    champs = df.groupby("ticker", dropna=False, as_index=False).apply(pick_fn)
    champs.index = range(len(champs))
    return champs

def summary_stats(df, by="model"):
    if df is None or df.empty:
        return pd.DataFrame()

    cols = [c for c in METRIC_ORDER if c in df.columns]
    if not cols:
        return pd.DataFrame()

    out = []
    for m in sorted(df[by].unique()):
        sub = df[df[by]==m]
        row = {"model": m, "n_rows": len(sub)}
        for c in cols:
            vals = pd.to_numeric(sub[c], errors="coerce").dropna()
            if len(vals) == 0:
                continue
            row[f"{c}_mean"] = float(np.mean(vals))
            row[f"{c}_std"]  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0
        out.append(row)
    return pd.DataFrame(out)

def head_to_head(champs_cnn, champs_lstm):
    """Merge per‑ticker champions from both models and compute differences."""
    if champs_cnn.empty or champs_lstm.empty:
        return pd.DataFrame()

    merged = pd.merge(
        champs_cnn.add_prefix("CNN_"),
        champs_lstm.add_prefix("LSTM_"),
        left_on="CNN_ticker", right_on="LSTM_ticker", how="inner"
    )
    # Compute deltas for overlapping metrics
    for m in METRIC_ORDER:
        cnn_col = f"CNN_{m}"
        lstm_col = f"LSTM_{m}"
        if cnn_col in merged.columns and lstm_col in merged.columns:
            # For accuracy-type metrics, positive delta means CNN better
            # For error metrics, negate sign so positive means CNN better too
            if m.startswith("RMSE") or m.startswith("MAE") or m.startswith("MAPE"):
                merged[f"delta_{m}"] = pd.to_numeric(merged[lstm_col], errors="coerce") - pd.to_numeric(merged[cnn_col], errors="coerce")
            else:
                merged[f"delta_{m}"] = pd.to_numeric(merged[cnn_col], errors="coerce") - pd.to_numeric(merged[lstm_col], errors="coerce")
    return merged

def make_bar_means(df_all, outdir):
    cols = [c for c in ["DA_test","RMSE_test","MAE_test","MAPE_test","PnL","PnL_ratio"] if c in df_all.columns]
    if not cols:
        return
    means = df_all.groupby("model")[cols].mean(numeric_only=True)
    if means.empty:
        return
    # One chart per metric
    outdir.mkdir(parents=True, exist_ok=True)
    for c in cols:
        plt.figure()
        means[c].plot(kind="bar")
        plt.title(f"Mean {c} by model")
        plt.ylabel(c)
        plt.xlabel("Model")
        plt.tight_layout()
        plt.savefig(outdir / f"mean_{c}_bars.png", dpi=150)
        plt.close()

def make_scatter_da(h2h, outdir):
    xcol, ycol = "LSTM_DA_test", "CNN_DA_test"
    if xcol not in h2h.columns or ycol not in h2h.columns:
        return
    outdir.mkdir(parents=True, exist_ok=True)
    xs = pd.to_numeric(h2h[xcol], errors="coerce")
    ys = pd.to_numeric(h2h[ycol], errors="coerce")
    plt.figure()
    plt.scatter(xs, ys)
    # y=x line
    lims = [min(min(xs.dropna(default=0), default=0), min(ys.dropna(default=0), default=0)),
            max(max(xs.dropna(default=1), default=1), max(ys.dropna(default=1), default=1))]
    plt.plot(lims, lims)
    plt.title("DA_test: CNN vs LSTM (per‑ticker champions)")
    plt.xlabel("LSTM DA_test")
    plt.ylabel("CNN DA_test")
    plt.tight_layout()
    plt.savefig(outdir / "scatter_DA_test_cnn_vs_lstm.png", dpi=150)
    plt.close()

def load_and_clean(path: Path, model_name: str) -> pd.DataFrame:
    if path is None or (not path.exists()):
        print(f"[warn] Missing results for {model_name}: {path}")
        return pd.DataFrame()

    df = pd.read_csv(path)
    if df.empty:
        print(f"[warn] Empty CSV for {model_name}: {path}")
        return df

    df = _normalize_columns(df)

    # Ensure minimal columns
    if "ticker" not in df.columns:
        # try to infer a single ticker from path (parent folder name or filename stem)
        tick_guess = path.stem.replace("lstm_results","").replace("cnn_results","").strip("_- ")
        df["ticker"] = tick_guess if tick_guess else "UNKNOWN"

    df["model"] = model_name
    return df

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--lstm", type=str, default=None, help="Path to LSTM results CSV")
    ap.add_argument("--cnn",  type=str, default=None, help="Path to CNN results CSV")
    ap.add_argument("--select_metric", type=str, default="DA_test", help="Metric to pick per‑ticker champions")
    ap.add_argument("--outdir", type=str, default="output/compare", help="Output folder for CSVs")
    ap.add_argument("--plotdir", type=str, default="output/compare_plots", help="Output folder for plots")
    args = ap.parse_args()

    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)
    plotdir = Path(args.plotdir); plotdir.mkdir(parents=True, exist_ok=True)

    # Resolve paths with fallbacks/auto‑discovery
    lstm_path = _find_paths(args.lstm, "result_file_lstm", "results/lstm_results.csv", "**/lstm_results*.csv")
    cnn_path  = _find_paths(args.cnn,  "result_file_cnn",  "results/cnn_results.csv",  "**/cnn_results*.csv")

    print(f"[info] LSTM CSV: {lstm_path}")
    print(f"[info] CNN  CSV: {cnn_path}")

    df_lstm = load_and_clean(lstm_path, "LSTM")
    df_cnn  = load_and_clean(cnn_path, "CNN")

    # Save unified cleaned copies
    if not df_lstm.empty:
        df_lstm.to_csv(outdir / "unified_lstm.csv", index=False)
    if not df_cnn.empty:
        df_cnn.to_csv(outdir / "unified_cnn.csv", index=False)

    # Concatenate for later stats/plots
    df_all = pd.concat([df_lstm, df_cnn], axis=0, ignore_index=True)

    # Pick champions per ticker per model
    champs_lstm = pick_champions(df_lstm, select_metric=args.select_metric)
    champs_cnn  = pick_champions(df_cnn,  select_metric=args.select_metric)

    if not champs_lstm.empty:
        champs_lstm.to_csv(outdir / "champions_lstm.csv", index=False)
    if not champs_cnn.empty:
        champs_cnn.to_csv(outdir / "champions_cnn.csv", index=False)

    # Head‑to‑head
    h2h = head_to_head(champs_cnn, champs_lstm)
    if not h2h.empty:
        h2h.to_csv(outdir / "head_to_head_by_ticker.csv", index=False)

    # Summary stats
    summ = summary_stats(df_all, by="model")
    if not summ.empty:
        summ.to_csv(outdir / "summary_stats.csv", index=False)

    # Plots
    if not df_all.empty:
        make_bar_means(df_all, plotdir)
    if not h2h.empty:
        make_scatter_da(h2h, plotdir)

    # Final console summary
    print("\n=== SUMMARY ===")
    if not summ.empty:
        print(summ.to_string(index=False))
    else:
        print("No summary stats (missing or empty CSVs).")

    if not h2h.empty and "delta_DA_test" in h2h.columns:
        mean_delta = float(pd.to_numeric(h2h["delta_DA_test"], errors="coerce").dropna().mean())
        print(f"\nMean delta_DA_test (CNN - LSTM) over champions: {mean_delta:.4f}")
    else:
        print("\nHead‑to‑head table not available.")

if __name__ == "__main__":
    main()
